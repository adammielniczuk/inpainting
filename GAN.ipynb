{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.transforms as T\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch.utils.data as dutils\n",
    "from tqdm import tqdm\n",
    "import torchvision.utils as vutils\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(dutils.Dataset):\n",
    "    def __init__(self, transform=None, Test=False):  \n",
    "        self.transform = transform\n",
    "        if Test:\n",
    "            self.folder_path='images/test/'\n",
    "        else:\n",
    "            self.folder_path='images/train/'\n",
    "        self.images = [x for x in os.listdir(self.folder_path) if 'cut' not in x if 'hole' not in x]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\t\n",
    "    def __getitem__(self, index):\n",
    "        image = self.images[index]\n",
    "        with Image.open(self.folder_path+image) as img:\n",
    "                if self.transform:\n",
    "                    img = self.transform(img)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.Compose([\n",
    "\tT.RandomHorizontalFlip(),\n",
    "    T.RandomVerticalFlip(),\n",
    "\tT.ToTensor()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset(transform=transform)\n",
    "test_ds = Dataset(transform=transform, Test=True)\n",
    "train = dutils.DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "test = dutils.DataLoader(test_ds, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.project = nn.Linear(256, 256**2)\n",
    "\t\tself.stack = nn.Sequential(\n",
    "\t\t\tnn.Conv2d(4, 32, 5, 1, 2),\n",
    "\t\t\tnn.ReLU(inplace=True),\n",
    "\t\t\tnn.MaxPool2d(2, 2),\n",
    "\n",
    "\n",
    "\t\t\tnn.Conv2d(32, 32*2, 5, 1, 2),\n",
    "\t\t\tnn.ReLU(inplace=True),\n",
    "\t\t\tnn.MaxPool2d(2, 2),\n",
    "\n",
    "\n",
    "\t\t\tnn.Conv2d(32*2, 32*4, 3, 1, 1),\n",
    "\t\t\tnn.ReLU(inplace=True),\n",
    "\t\t\tnn.MaxPool2d(2, 2),\n",
    "\t\n",
    "\n",
    "\t\t\tnn.Conv2d(32*4, 32*8, 3, 1, 1),\n",
    "\t\t\tnn.ReLU(inplace=True),\n",
    "\t\t\tnn.MaxPool2d(2, 2),\n",
    "\t\t\n",
    "\t\t\tnn.Conv2d(32*8, 32*16, 3, 1, 1),\n",
    "\t\t\tnn.ReLU(inplace=True),\n",
    "\t\t\tnn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(32*16, 32*32, 3, 1, 1),\n",
    "\t\t\tnn.ReLU(inplace=True),\n",
    "\t\t\tnn.MaxPool2d(2, 2),\n",
    "\t\t\t\n",
    "            nn.ConvTranspose2d(32*32, 32*16, 4, 2, 1),\n",
    "\t\t\tnn.ReLU(inplace=True),\n",
    "\n",
    "\t\t\tnn.ConvTranspose2d(32*16, 32*8, 4, 2, 1),\n",
    "\t\t\tnn.ReLU(inplace=True),\n",
    "\t\t\t\n",
    "\n",
    "\t\t\tnn.ConvTranspose2d(32*8, 32*4, 4, 2, 1),\n",
    "\t\t\tnn.ReLU(inplace=True),\n",
    "\t\n",
    "\n",
    "\t\t\tnn.ConvTranspose2d(32*4, 32*2, 4, 2, 1),\n",
    "\t\t\tnn.ReLU(inplace=True),\n",
    "\t\t\tnn.ConvTranspose2d(32*2, 32, 4, 2, 1),\n",
    "\t\t\tnn.ReLU(inplace=True),\n",
    "\t\t\t\n",
    "\n",
    "\t\t\tnn.ConvTranspose2d(32, 3, 4, 2, 1),\n",
    "\t\t\tnn.Sigmoid()\n",
    "\t\t\t\n",
    "\t\t)\n",
    "\t\n",
    "\tdef forward(self, x, z):\n",
    "\t\tb_size = x.shape[0]\n",
    "\t\tcond = self.project(z)\n",
    "\t\tcond = torch.reshape(cond, (b_size, 1, 256, 256))\n",
    "\t\tx = torch.concat((cond, x), dim=1)\n",
    "\t\treturn self.stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "model = Generator()\n",
    "\n",
    "x = torch.randn(16, 3, 256, 256)\n",
    "z = torch.randn(16, 256)\n",
    "\n",
    "images = model(x, z)\n",
    "\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(Discriminator, self).__init__()\n",
    "\t\tself.main = nn.Sequential(\n",
    "\t\t\tnn.Conv2d(3, 32, 4, 2, 1),\n",
    "\t\t\tnn.LeakyReLU(0.2, inplace=True),\n",
    "\t\t\t\n",
    "\t\t\tnn.Conv2d(32, 32*2, 4, 2, 1),\n",
    "\t\t\tnn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "\t\t\tnn.Conv2d(32*2, 32*4, 4, 2, 1),\n",
    "\t\t\tnn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "\t\t\tnn.Conv2d(32*4, 32*8, 4, 2, 1),\n",
    "\t\t\tnn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "\t\t\tnn.Conv2d(32*8, 32*16, 4, 2, 1),\n",
    "\t\t\tnn.LeakyReLU(0.2, inplace=True),\n",
    "\t\t\t\n",
    "            nn.Conv2d(32*16, 32*32, 4, 2, 1),\n",
    "\t\t\tnn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "\t\t\tnn.Conv2d(32*32, 1, 4, 1, 0),\n",
    "\t\t\tnn.Sigmoid()\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, input):\n",
    "\t\treturn self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "\t'cuda' if torch.cuda.is_available()\n",
    "\telse 'mps' if torch.backends.mps.is_available()\n",
    "\telse 'cpu'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "Z_DIM = 256\n",
    "EPOCHS = 300\n",
    "BETA_1 = 0.5\n",
    "BETA_2 = 0.999\n",
    "LEARNING_RATE = 1e-4\n",
    "FEATURES_D = 32\n",
    "FEATURES_G = 32\n",
    "IMG_SIZE = 256\n",
    "PATCH_SIZE = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.1\n"
     ]
    }
   ],
   "source": [
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started training using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|â–Š         | 70/845 [00:15<02:55,  4.42it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10272/2687330870.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mimage_batch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m                 \u001b[0mimage_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m                 \u001b[0mb_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m                 \u001b[0mdiscriminator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(f'Started training using device: {device}')\n",
    "\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "d_opt = torch.optim.Adam(discriminator.parameters(), lr=LEARNING_RATE, betas=(BETA_1, BETA_2))\n",
    "g_opt = torch.optim.Adam(generator.parameters(), lr=LEARNING_RATE, betas=(BETA_1, BETA_2))\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "fixed_noise = torch.randn(16, Z_DIM, device=device)\n",
    "fixed_images = next(iter(train))[:16].to(device)\n",
    "\n",
    "fixed_x_offset = max(0, (IMG_SIZE - 32) // 2)\n",
    "fixed_y_offset =  min(IMG_SIZE, fixed_x_offset + 32)\n",
    "start = time.time()\n",
    "for epoch in range(EPOCHS):\n",
    "\tfor image_batch in tqdm(train):\n",
    "\t\timage_batch = image_batch.to(device)\n",
    "\t\tb_size = image_batch.shape[0]\n",
    "\t\tdiscriminator.zero_grad()\n",
    "\n",
    "\t\ty_hat_real = discriminator(image_batch).view(-1)\n",
    "\t\ty_real = torch.ones_like(y_hat_real, device=device)\n",
    "\t\treal_loss = loss_fn(y_hat_real, y_real)\n",
    "\t\treal_loss.backward()\n",
    "\n",
    "\t\n",
    "\n",
    "\t\timage_batch[:, :, fixed_x_offset:fixed_x_offset+PATCH_SIZE, fixed_y_offset:fixed_y_offset+PATCH_SIZE] = 0\n",
    "\n",
    "\t\t# Predict using generator\n",
    "\t\tnoise = torch.randn(b_size, Z_DIM, device=device)\n",
    "\t\tpredicted_patch = generator(image_batch, noise)\n",
    "\n",
    "\t\t# Replace black patch with generator output\n",
    "\t\timage_batch[:, :, fixed_x_offset:fixed_x_offset+PATCH_SIZE, fixed_y_offset:fixed_y_offset+PATCH_SIZE] = predicted_patch[:, :, fixed_x_offset:fixed_x_offset+PATCH_SIZE, fixed_y_offset:fixed_y_offset+PATCH_SIZE]\n",
    "\n",
    "\t\t# Predict fake images using discriminator\n",
    "\t\ty_hat_fake = discriminator(image_batch.detach()).view(-1)\n",
    "\n",
    "\t\t# Train discriminator\n",
    "\t\ty_fake = torch.zeros_like(y_hat_fake)\n",
    "\t\tfake_loss = loss_fn(y_hat_fake, y_fake)\n",
    "\t\tfake_loss.backward()\n",
    "\t\td_opt.step()\n",
    "\n",
    "\t\t# Train generator\n",
    "\t\tgenerator.zero_grad()\n",
    "\t\ty_hat_fake = discriminator(image_batch).view(-1)\n",
    "\t\tg_loss = loss_fn(y_hat_fake, torch.ones_like(y_hat_fake))\n",
    "\t\tg_loss.backward()\n",
    "\t\tg_opt.step()\n",
    "\n",
    "\tfixed_images[:, :, fixed_x_offset:fixed_x_offset+PATCH_SIZE, fixed_y_offset:fixed_y_offset+PATCH_SIZE] = 0\n",
    "\twith torch.no_grad():\n",
    "\t\tpredicted_patches = generator(fixed_images, fixed_noise)\n",
    "\tfixed_images[:, :, fixed_x_offset:fixed_x_offset+PATCH_SIZE, fixed_y_offset:fixed_y_offset+PATCH_SIZE] = predicted_patches\n",
    "\timg = T.ToPILImage()(vutils.make_grid(fixed_images.to('cpu'), normalize=True, padding=2, nrow=4))\n",
    "\timg.save(f'progress/epoch_{epoch}.jpg')\n",
    "train_time = time.time() - start\n",
    "print(f'Total training time: {train_time // 60} minutes')\n",
    "\n",
    "generator = generator.to('cpu')\n",
    "\n",
    "torch.save(generator, 'models/patch_generator.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
